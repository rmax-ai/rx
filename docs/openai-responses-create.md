# Responses API — Create a response

`POST https://api.openai.com/v1/responses`

## Purpose

Invoke the Responses API to generate multi-modal output (text, images, JSON, tool calls, etc.) given developer-managed inputs. The request can prepend conversation state, hand off to built-in tools (web search, file search, code interpreter, computer use), or run your own custom tool via functions during the same response.

## Key request parameters

| Parameter | Description |
| --- | --- |
| `model` (required) | The model to use for the response (e.g., `gpt-5.1-mini`). A high-reliability, general-purpose reasoning model is recommended for autonomous agents. |
| `input` | A string, array of typed `ResponseInputItem`s, or richer `message` objects that include text, images, files, or prior assistant replies. Use `developer`/`system` roles to enforce instructions. |
| `conversation` | Optional conversation identifier so items are appended to an existing conversation (items prepended to the next request). Supports either a simple ID string or `{ "id": "..." }`. |
| `include` | Array of special keys (e.g., `"web_search_call.action.sources"`, `"message.output_text.logprobs"`, `"reasoning.encrypted_content"`) to request extra metadata such as tool call sources or encrypted reasoning for deterministic replay. |
| `context_management` | Optional list of compaction rules (`type: "compaction"` with `compact_threshold`) to automatically summarize long histories before token limits are hit. |
| `background` | If true, the response runs asynchronously (background work), returning immediately with status metadata; poll the returned `id` via `/responses/{id}`. |

## Important response fields

| Field | Description |
| --- | --- |
| `id` | Response identifier; use it to track progress, poll for completion, and correlate logs. |
| `status` | `"completed"`, `"in_progress"`, `"incomplete"`, etc. Indicates whether the response finished or still has pending items/tool calls. |
| `output` | Contains `items` such as `message`, `tool_call`, `reasoning`, `compaction`, etc. Messages expose `content` arrays with text/annotations/citations/logprobs or refusal data. |
| `items` | Low-level stream of things generated by the model (tool calls, reasoning, file search results, etc.). Monitor `status` on tool items to pass outputs back into your workflow (e.g., `code_interpreter_call`, `web_search_call`). |
| `pending_tool_calls`, `tool_call` outputs | If the model invokes a tool, expect `pending_safety_checks` or `call_id`s; send the results back via the tool execution APIs before resuming. |

## Usage tips & reminders

- Always include `model` and normalize `input` to mitigate prompt leakage. Use the `developer` role for instructions that should override user content.
- Request only the `include` values you actually need; some add extra latency or storage cost (e.g., encrypted reasoning is large and intended for stateless replay).
- Track the returned `id`/`request_id` headers for telemetry and retries. For asynchronous or long-running responses, poll `/responses/{id}` instead of reissuing the request.
- When tooling is involved, examine each `tool_call` item: supply outputs (logs, files, structured JSON) as new inputs to the next `responses` request to keep state consistent.
- Respect rate limits and concurrency; handle non-2xx responses by surfacing the OpenAI error envelope (`error.message`, `type`, `param`, `code`) for debugging.

## Reference

[OpenAI Responses API — Create a response](https://developers.openai.com/api/reference/resources/responses/methods/create)
